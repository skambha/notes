scala> val decimalStr = "1" + "0" * 19
decimalStr: String = 10000000000000000000

scala> val df = spark.range(0, 1, 1, 1).union(spark.range(0, 11, 1, 1))
df: org.apache.spark.sql.Dataset[Long] = [id: bigint]

scala> df.select(expr(s"cast('$decimalStr' as decimal (38, 18)) as d"), lit(1).as("key")).groupBy("key").agg(sum($"d"))
res1: org.apache.spark.sql.DataFrame = [key: int, sum(d): decimal(38,18)]

scala> df.select(expr(s"cast('$decimalStr' as decimal (38, 18)) as d"), lit(1).as("key")).groupBy("key").agg(sum($"d")).show
20/03/10 15:01:18 ERROR Executor: Exception in task 1.0 in stage 0.0 (TID 1)/ 2]
java.lang.ArithmeticException: Decimal precision 39 exceeds max precision 38
	at org.apache.spark.sql.types.Decimal.set(Decimal.scala:122)
	at org.apache.spark.sql.types.Decimal$.apply(Decimal.scala:598)
	at org.apache.spark.sql.types.Decimal.apply(Decimal.scala)
	at org.apache.spark.sql.catalyst.expressions.UnsafeRow.getDecimal(UnsafeRow.java:393)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.agg_doConsume_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:460)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:463)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/03/10 15:01:18 ERROR TaskSetManager: Task 1 in stage 0.0 failed 1 times; aborting job
org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1, 10.0.0.118, executor driver): java.lang.ArithmeticException: Decimal precision 39 exceeds max precision 38
	at org.apache.spark.sql.types.Decimal.set(Decimal.scala:122)
	at org.apache.spark.sql.types.Decimal$.apply(Decimal.scala:598)
	at org.apache.spark.sql.types.Decimal.apply(Decimal.scala)
	at org.apache.spark.sql.catalyst.expressions.UnsafeRow.getDecimal(UnsafeRow.java:393)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.agg_doConsume_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:460)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:463)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2026)
  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1975)
  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1974)
  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1974)
  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:953)
  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:953)
  at scala.Option.foreach(Option.scala:407)
  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:953)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2206)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2155)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2144)
  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:755)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2104)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2125)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2144)
  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:467)
  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:420)
  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)
  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3556)
  at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2609)
  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3546)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)
  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)
  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3544)
  at org.apache.spark.sql.Dataset.head(Dataset.scala:2609)
  at org.apache.spark.sql.Dataset.take(Dataset.scala:2816)
  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:298)
  at org.apache.spark.sql.Dataset.showString(Dataset.scala:335)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:822)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:781)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:790)
  ... 47 elided
Caused by: java.lang.ArithmeticException: Decimal precision 39 exceeds max precision 38
  at org.apache.spark.sql.types.Decimal.set(Decimal.scala:122)
  at org.apache.spark.sql.types.Decimal$.apply(Decimal.scala:598)
  at org.apache.spark.sql.types.Decimal.apply(Decimal.scala)
  at org.apache.spark.sql.catalyst.expressions.UnsafeRow.getDecimal(UnsafeRow.java:393)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.agg_doConsume_0$(Unknown Source)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.agg_doAggregateWithKeys_0$(Unknown Source)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
  at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
  at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)
  at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)
  at org.apache.spark.scheduler.Task.run(Task.scala:127)
  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:460)
  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:463)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  at java.lang.Thread.run(Thread.java:748)

scala> val df1 = df.select(expr(s"cast('$decimalStr' as decimal (38, 18)) as d"), lit(1).as("key")).groupBy("key").agg(sum($"d"))
df1: org.apache.spark.sql.DataFrame = [key: int, sum(d): decimal(38,18)]

scala> df1.explain(true)
== Parsed Logical Plan ==
'Aggregate [key#48], [key#48, sum('d) AS sum(d)#54]
+- Project [cast(10000000000000000000 as decimal(38,18)) AS d#47, 1 AS key#48]
   +- Union
      :- Range (0, 1, step=1, splits=Some(1))
      +- Range (0, 11, step=1, splits=Some(1))

== Analyzed Logical Plan ==
key: int, sum(d): decimal(38,18)
Aggregate [key#48], [key#48, sum(d#47) AS sum(d)#54]
+- Project [cast(10000000000000000000 as decimal(38,18)) AS d#47, 1 AS key#48]
   +- Union
      :- Range (0, 1, step=1, splits=Some(1))
      +- Range (0, 11, step=1, splits=Some(1))

== Optimized Logical Plan ==
Aggregate [key#48], [key#48, sum(d#47) AS sum(d)#54]
+- Union
   :- Project [10000000000000000000.000000000000000000 AS d#47, 1 AS key#48]
   :  +- Range (0, 1, step=1, splits=Some(1))
   +- Project [10000000000000000000.000000000000000000 AS d#57, 1 AS key#58]
      +- Range (0, 11, step=1, splits=Some(1))

== Physical Plan ==
*(4) HashAggregate(keys=[key#48], functions=[sum(d#47)], output=[key#48, sum(d)#54])
+- Exchange hashpartitioning(key#48, 200), true, [id=#77]
   +- *(3) HashAggregate(keys=[key#48], functions=[partial_sum(d#47)], output=[key#48, sum#60])
      +- Union
         :- *(1) Project [10000000000000000000.000000000000000000 AS d#47, 1 AS key#48]
         :  +- *(1) Range (0, 1, step=1, splits=1)
         +- *(2) Project [10000000000000000000.000000000000000000 AS d#57, 1 AS key#58]
            +- *(2) Range (0, 11, step=1, splits=1)


scala> df1.queryExecution
   val queryExecution: org.apache.spark.sql.execution.QueryExecution

scala> df1.queryExecution.debug.codegen
Found 4 WholeStageCodegen subtrees.
== Subtree 1 / 4 (maxMethodCodeSize:209; maxConstantPoolSize:229(0.35% used); numInnerClasses:0) ==
*(4) HashAggregate(keys=[key#48], functions=[sum(d#47)], output=[key#48, sum(d)#54])
+- Exchange hashpartitioning(key#48, 200), true, [id=#77]
   +- *(3) HashAggregate(keys=[key#48], functions=[partial_sum(d#47)], output=[key#48, sum#60])
      +- Union
         :- *(1) Project [10000000000000000000.000000000000000000 AS d#47, 1 AS key#48]
         :  +- *(1) Range (0, 1, step=1, splits=1)
         +- *(2) Project [10000000000000000000.000000000000000000 AS d#57, 1 AS key#58]
            +- *(2) Range (0, 11, step=1, splits=1)

Generated code:
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage4(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=4
/* 006 */ final class GeneratedIteratorForCodegenStage4 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean agg_initAgg_0;
/* 010 */   private org.apache.spark.unsafe.KVIterator agg_mapIter_0;
/* 011 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap agg_hashMap_0;
/* 012 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter agg_sorter_0;
/* 013 */   private scala.collection.Iterator inputadapter_input_0;
/* 014 */   private boolean agg_agg_isNull_2_0;
/* 015 */   private boolean agg_agg_isNull_4_0;
/* 016 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] agg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 017 */
/* 018 */   public GeneratedIteratorForCodegenStage4(Object[] references) {
/* 019 */     this.references = references;
/* 020 */   }
/* 021 */
/* 022 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 023 */     partitionIndex = index;
/* 024 */     this.inputs = inputs;
/* 025 */
/* 026 */     inputadapter_input_0 = inputs[0];
/* 027 */     agg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 028 */     agg_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 029 */
/* 030 */   }
/* 031 */
/* 032 */   private void agg_doAggregate_sum_0(org.apache.spark.sql.catalyst.InternalRow agg_unsafeRowAggBuffer_0, org.apache.spark.sql.types.Decimal agg_expr_1_0, boolean agg_exprIsNull_1_0) throws java.io.IOException {
/* 033 */     // evaluate aggregate function for sum
/* 034 */     agg_agg_isNull_2_0 = true;
/* 035 */     Decimal agg_value_2 = null;
/* 036 */     do {
/* 037 */       boolean agg_isNull_3 = true;
/* 038 */       Decimal agg_value_3 = null;
/* 039 */       agg_agg_isNull_4_0 = true;
/* 040 */       Decimal agg_value_4 = null;
/* 041 */       do {
/* 042 */         boolean agg_isNull_5 = agg_unsafeRowAggBuffer_0.isNullAt(0);
/* 043 */         Decimal agg_value_5 = agg_isNull_5 ?
/* 044 */         null : (agg_unsafeRowAggBuffer_0.getDecimal(0, 38, 18));
/* 045 */         if (!agg_isNull_5) {
/* 046 */           agg_agg_isNull_4_0 = false;
/* 047 */           agg_value_4 = agg_value_5;
/* 048 */           continue;
/* 049 */         }
/* 050 */
/* 051 */         if (!false) {
/* 052 */           agg_agg_isNull_4_0 = false;
/* 053 */           agg_value_4 = ((Decimal) references[4] /* literal */);
/* 054 */           continue;
/* 055 */         }
/* 056 */
/* 057 */       } while (false);
/* 058 */
/* 059 */       if (!agg_exprIsNull_1_0) {
/* 060 */         agg_isNull_3 = false; // resultCode could change nullability.
/* 061 */         agg_value_3 = agg_value_4.$plus(agg_expr_1_0);
/* 062 */
/* 063 */       }
/* 064 */       if (!agg_isNull_3) {
/* 065 */         agg_agg_isNull_2_0 = false;
/* 066 */         agg_value_2 = agg_value_3;
/* 067 */         continue;
/* 068 */       }
/* 069 */
/* 070 */       boolean agg_isNull_8 = agg_unsafeRowAggBuffer_0.isNullAt(0);
/* 071 */       Decimal agg_value_8 = agg_isNull_8 ?
/* 072 */       null : (agg_unsafeRowAggBuffer_0.getDecimal(0, 38, 18));
/* 073 */       if (!agg_isNull_8) {
/* 074 */         agg_agg_isNull_2_0 = false;
/* 075 */         agg_value_2 = agg_value_8;
/* 076 */         continue;
/* 077 */       }
/* 078 */
/* 079 */     } while (false);
/* 080 */     // update unsafe row buffer
/* 081 */     if (!agg_agg_isNull_2_0) {
/* 082 */       agg_unsafeRowAggBuffer_0.setDecimal(0, agg_value_2, 38);
/* 083 */     } else {
/* 084 */       agg_unsafeRowAggBuffer_0.setDecimal(0, null, 38);
/* 085 */     }
/* 086 */   }
/* 087 */
/* 088 */   private void agg_doAggregateWithKeysOutput_0(UnsafeRow agg_keyTerm_0, UnsafeRow agg_bufferTerm_0)
/* 089 */   throws java.io.IOException {
/* 090 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* numOutputRows */).add(1);
/* 091 */
/* 092 */     int agg_value_9 = agg_keyTerm_0.getInt(0);
/* 093 */     boolean agg_isNull_10 = agg_bufferTerm_0.isNullAt(0);
/* 094 */     Decimal agg_value_10 = agg_isNull_10 ?
/* 095 */     null : (agg_bufferTerm_0.getDecimal(0, 38, 18));
/* 096 */     boolean agg_isNull_11 = agg_isNull_10;
/* 097 */     Decimal agg_value_11 = null;
/* 098 */
/* 099 */     if (!agg_isNull_10) {
/* 100 */       agg_value_11 = agg_value_10.toPrecision(
/* 101 */         38, 18, Decimal.ROUND_HALF_UP(), true);
/* 102 */       agg_isNull_11 = agg_value_11 == null;
/* 103 */
/* 104 */     }
/* 105 */
/* 106 */     agg_mutableStateArray_0[1].reset();
/* 107 */
/* 108 */     agg_mutableStateArray_0[1].zeroOutNullBytes();
/* 109 */
/* 110 */     agg_mutableStateArray_0[1].write(0, agg_value_9);
/* 111 */
/* 112 */     if (agg_isNull_11) {
/* 113 */       agg_mutableStateArray_0[1].write(1, (Decimal) null, 38, 18);
/* 114 */     } else {
/* 115 */       agg_mutableStateArray_0[1].write(1, agg_value_11, 38, 18);
/* 116 */     }
/* 117 */     append((agg_mutableStateArray_0[1].getRow()));
/* 118 */
/* 119 */   }
/* 120 */
/* 121 */   private void agg_doConsume_0(InternalRow inputadapter_row_0, int agg_expr_0_0, Decimal agg_expr_1_0, boolean agg_exprIsNull_1_0) throws java.io.IOException {
/* 122 */     UnsafeRow agg_unsafeRowAggBuffer_0 = null;
/* 123 */
/* 124 */     // generate grouping key
/* 125 */     agg_mutableStateArray_0[0].reset();
/* 126 */
/* 127 */     agg_mutableStateArray_0[0].write(0, agg_expr_0_0);
/* 128 */     int agg_unsafeRowKeyHash_0 = (agg_mutableStateArray_0[0].getRow()).hashCode();
/* 129 */     if (true) {
/* 130 */       // try to get the buffer from hash map
/* 131 */       agg_unsafeRowAggBuffer_0 =
/* 132 */       agg_hashMap_0.getAggregationBufferFromUnsafeRow((agg_mutableStateArray_0[0].getRow()), agg_unsafeRowKeyHash_0);
/* 133 */     }
/* 134 */     // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based
/* 135 */     // aggregation after processing all input rows.
/* 136 */     if (agg_unsafeRowAggBuffer_0 == null) {
/* 137 */       if (agg_sorter_0 == null) {
/* 138 */         agg_sorter_0 = agg_hashMap_0.destructAndCreateExternalSorter();
/* 139 */       } else {
/* 140 */         agg_sorter_0.merge(agg_hashMap_0.destructAndCreateExternalSorter());
/* 141 */       }
/* 142 */
/* 143 */       // the hash map had be spilled, it should have enough memory now,
/* 144 */       // try to allocate buffer again.
/* 145 */       agg_unsafeRowAggBuffer_0 = agg_hashMap_0.getAggregationBufferFromUnsafeRow(
/* 146 */         (agg_mutableStateArray_0[0].getRow()), agg_unsafeRowKeyHash_0);
/* 147 */       if (agg_unsafeRowAggBuffer_0 == null) {
/* 148 */         // failed to allocate the first page
/* 149 */         throw new org.apache.spark.memory.SparkOutOfMemoryError("No enough memory for aggregation");
/* 150 */       }
/* 151 */     }
/* 152 */
/* 153 */     // common sub-expressions
/* 154 */
/* 155 */     // evaluate aggregate functions and update aggregation buffers
/* 156 */     agg_doAggregate_sum_0(agg_unsafeRowAggBuffer_0, agg_expr_1_0, agg_exprIsNull_1_0);
/* 157 */
/* 158 */   }
/* 159 */
/* 160 */   private void agg_doAggregateWithKeys_0() throws java.io.IOException {
/* 161 */     while ( inputadapter_input_0.hasNext()) {
/* 162 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 163 */
/* 164 */       int inputadapter_value_0 = inputadapter_row_0.getInt(0);
/* 165 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);
/* 166 */       Decimal inputadapter_value_1 = inputadapter_isNull_1 ?
/* 167 */       null : (inputadapter_row_0.getDecimal(1, 38, 18));
/* 168 */
/* 169 */       agg_doConsume_0(inputadapter_row_0, inputadapter_value_0, inputadapter_value_1, inputadapter_isNull_1);
/* 170 */       // shouldStop check is eliminated
/* 171 */     }
/* 172 */
/* 173 */     agg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(agg_hashMap_0, agg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* avgHashProbe */));
/* 174 */   }
/* 175 */
/* 176 */   protected void processNext() throws java.io.IOException {
/* 177 */     if (!agg_initAgg_0) {
/* 178 */       agg_initAgg_0 = true;
/* 179 */
/* 180 */       agg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();
/* 181 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();
/* 182 */       agg_doAggregateWithKeys_0();
/* 183 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);
/* 184 */     }
/* 185 */     // output the result
/* 186 */
/* 187 */     while ( agg_mapIter_0.next()) {
/* 188 */       UnsafeRow agg_aggKey_0 = (UnsafeRow) agg_mapIter_0.getKey();
/* 189 */       UnsafeRow agg_aggBuffer_0 = (UnsafeRow) agg_mapIter_0.getValue();
/* 190 */       agg_doAggregateWithKeysOutput_0(agg_aggKey_0, agg_aggBuffer_0);
/* 191 */       if (shouldStop()) return;
/* 192 */     }
/* 193 */     agg_mapIter_0.close();
/* 194 */     if (agg_sorter_0 == null) {
/* 195 */       agg_hashMap_0.free();
/* 196 */     }
/* 197 */   }
/* 198 */
/* 199 */ }

== Subtree 2 / 4 (maxMethodCodeSize:309; maxConstantPoolSize:216(0.33% used); numInnerClasses:0) ==
*(3) HashAggregate(keys=[key#48], functions=[partial_sum(d#47)], output=[key#48, sum#60])
+- Union
   :- *(1) Project [10000000000000000000.000000000000000000 AS d#47, 1 AS key#48]
   :  +- *(1) Range (0, 1, step=1, splits=1)
   +- *(2) Project [10000000000000000000.000000000000000000 AS d#57, 1 AS key#58]
      +- *(2) Range (0, 11, step=1, splits=1)

Generated code:
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage3(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=3
/* 006 */ final class GeneratedIteratorForCodegenStage3 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean agg_initAgg_0;
/* 010 */   private org.apache.spark.unsafe.KVIterator agg_mapIter_0;
/* 011 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap agg_hashMap_0;
/* 012 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter agg_sorter_0;
/* 013 */   private scala.collection.Iterator inputadapter_input_0;
/* 014 */   private boolean agg_agg_isNull_2_0;
/* 015 */   private boolean agg_agg_isNull_4_0;
/* 016 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] agg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 017 */
/* 018 */   public GeneratedIteratorForCodegenStage3(Object[] references) {
/* 019 */     this.references = references;
/* 020 */   }
/* 021 */
/* 022 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 023 */     partitionIndex = index;
/* 024 */     this.inputs = inputs;
/* 025 */
/* 026 */     inputadapter_input_0 = inputs[0];
/* 027 */     agg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 028 */     agg_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 029 */
/* 030 */   }
/* 031 */
/* 032 */   private void agg_doAggregateWithKeysOutput_0(UnsafeRow agg_keyTerm_0, UnsafeRow agg_bufferTerm_0)
/* 033 */   throws java.io.IOException {
/* 034 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* numOutputRows */).add(1);
/* 035 */
/* 036 */     int agg_value_9 = agg_keyTerm_0.getInt(0);
/* 037 */     boolean agg_isNull_10 = agg_bufferTerm_0.isNullAt(0);
/* 038 */     Decimal agg_value_10 = agg_isNull_10 ?
/* 039 */     null : (agg_bufferTerm_0.getDecimal(0, 38, 18));
/* 040 */
/* 041 */     agg_mutableStateArray_0[1].reset();
/* 042 */
/* 043 */     agg_mutableStateArray_0[1].zeroOutNullBytes();
/* 044 */
/* 045 */     agg_mutableStateArray_0[1].write(0, agg_value_9);
/* 046 */
/* 047 */     if (agg_isNull_10) {
/* 048 */       agg_mutableStateArray_0[1].write(1, (Decimal) null, 38, 18);
/* 049 */     } else {
/* 050 */       agg_mutableStateArray_0[1].write(1, agg_value_10, 38, 18);
/* 051 */     }
/* 052 */     append((agg_mutableStateArray_0[1].getRow()));
/* 053 */
/* 054 */   }
/* 055 */
/* 056 */   private void agg_doConsume_0(InternalRow inputadapter_row_0, Decimal agg_expr_0_0, int agg_expr_1_0) throws java.io.IOException {
/* 057 */     UnsafeRow agg_unsafeRowAggBuffer_0 = null;
/* 058 */
/* 059 */     // generate grouping key
/* 060 */     agg_mutableStateArray_0[0].reset();
/* 061 */
/* 062 */     agg_mutableStateArray_0[0].write(0, agg_expr_1_0);
/* 063 */     int agg_unsafeRowKeyHash_0 = (agg_mutableStateArray_0[0].getRow()).hashCode();
/* 064 */     if (true) {
/* 065 */       // try to get the buffer from hash map
/* 066 */       agg_unsafeRowAggBuffer_0 =
/* 067 */       agg_hashMap_0.getAggregationBufferFromUnsafeRow((agg_mutableStateArray_0[0].getRow()), agg_unsafeRowKeyHash_0);
/* 068 */     }
/* 069 */     // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based
/* 070 */     // aggregation after processing all input rows.
/* 071 */     if (agg_unsafeRowAggBuffer_0 == null) {
/* 072 */       if (agg_sorter_0 == null) {
/* 073 */         agg_sorter_0 = agg_hashMap_0.destructAndCreateExternalSorter();
/* 074 */       } else {
/* 075 */         agg_sorter_0.merge(agg_hashMap_0.destructAndCreateExternalSorter());
/* 076 */       }
/* 077 */
/* 078 */       // the hash map had be spilled, it should have enough memory now,
/* 079 */       // try to allocate buffer again.
/* 080 */       agg_unsafeRowAggBuffer_0 = agg_hashMap_0.getAggregationBufferFromUnsafeRow(
/* 081 */         (agg_mutableStateArray_0[0].getRow()), agg_unsafeRowKeyHash_0);
/* 082 */       if (agg_unsafeRowAggBuffer_0 == null) {
/* 083 */         // failed to allocate the first page
/* 084 */         throw new org.apache.spark.memory.SparkOutOfMemoryError("No enough memory for aggregation");
/* 085 */       }
/* 086 */     }
/* 087 */
/* 088 */     // common sub-expressions
/* 089 */
/* 090 */     // evaluate aggregate functions and update aggregation buffers
/* 091 */
/* 092 */     // evaluate aggregate function for sum
/* 093 */     agg_agg_isNull_2_0 = true;
/* 094 */     Decimal agg_value_2 = null;
/* 095 */     do {
/* 096 */       agg_agg_isNull_4_0 = true;
/* 097 */       Decimal agg_value_4 = null;
/* 098 */       do {
/* 099 */         boolean agg_isNull_5 = agg_unsafeRowAggBuffer_0.isNullAt(0);
/* 100 */         Decimal agg_value_5 = agg_isNull_5 ?
/* 101 */         null : (agg_unsafeRowAggBuffer_0.getDecimal(0, 38, 18));
/* 102 */         if (!agg_isNull_5) {
/* 103 */           agg_agg_isNull_4_0 = false;
/* 104 */           agg_value_4 = agg_value_5;
/* 105 */           continue;
/* 106 */         }
/* 107 */
/* 108 */         if (!false) {
/* 109 */           agg_agg_isNull_4_0 = false;
/* 110 */           agg_value_4 = ((Decimal) references[4] /* literal */);
/* 111 */           continue;
/* 112 */         }
/* 113 */
/* 114 */       } while (false);
/* 115 */
/* 116 */       Decimal agg_value_3 = null;
/* 117 */       agg_value_3 = agg_value_4.$plus(agg_expr_0_0);
/* 118 */       if (!false) {
/* 119 */         agg_agg_isNull_2_0 = false;
/* 120 */         agg_value_2 = agg_value_3;
/* 121 */         continue;
/* 122 */       }
/* 123 */
/* 124 */       boolean agg_isNull_8 = agg_unsafeRowAggBuffer_0.isNullAt(0);
/* 125 */       Decimal agg_value_8 = agg_isNull_8 ?
/* 126 */       null : (agg_unsafeRowAggBuffer_0.getDecimal(0, 38, 18));
/* 127 */       if (!agg_isNull_8) {
/* 128 */         agg_agg_isNull_2_0 = false;
/* 129 */         agg_value_2 = agg_value_8;
/* 130 */         continue;
/* 131 */       }
/* 132 */
/* 133 */     } while (false);
/* 134 */     // update unsafe row buffer
/* 135 */     agg_unsafeRowAggBuffer_0.setDecimal(0, agg_value_2, 38);
/* 136 */
/* 137 */   }
/* 138 */
/* 139 */   private void agg_doAggregateWithKeys_0() throws java.io.IOException {
/* 140 */     while ( inputadapter_input_0.hasNext()) {
/* 141 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 142 */
/* 143 */       Decimal inputadapter_value_0 = inputadapter_row_0.getDecimal(0, 38, 18);
/* 144 */       int inputadapter_value_1 = inputadapter_row_0.getInt(1);
/* 145 */
/* 146 */       agg_doConsume_0(inputadapter_row_0, inputadapter_value_0, inputadapter_value_1);
/* 147 */       // shouldStop check is eliminated
/* 148 */     }
/* 149 */
/* 150 */     agg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(agg_hashMap_0, agg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* avgHashProbe */));
/* 151 */   }
/* 152 */
/* 153 */   protected void processNext() throws java.io.IOException {
/* 154 */     if (!agg_initAgg_0) {
/* 155 */       agg_initAgg_0 = true;
/* 156 */
/* 157 */       agg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();
/* 158 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();
/* 159 */       agg_doAggregateWithKeys_0();
/* 160 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);
/* 161 */     }
/* 162 */     // output the result
/* 163 */
/* 164 */     while ( agg_mapIter_0.next()) {
/* 165 */       UnsafeRow agg_aggKey_0 = (UnsafeRow) agg_mapIter_0.getKey();
/* 166 */       UnsafeRow agg_aggBuffer_0 = (UnsafeRow) agg_mapIter_0.getValue();
/* 167 */       agg_doAggregateWithKeysOutput_0(agg_aggKey_0, agg_aggBuffer_0);
/* 168 */       if (shouldStop()) return;
/* 169 */     }
/* 170 */     agg_mapIter_0.close();
/* 171 */     if (agg_sorter_0 == null) {
/* 172 */       agg_hashMap_0.free();
/* 173 */     }
/* 174 */   }
/* 175 */
/* 176 */ }

== Subtree 3 / 4 (maxMethodCodeSize:280; maxConstantPoolSize:177(0.27% used); numInnerClasses:0) ==
*(2) Project [10000000000000000000.000000000000000000 AS d#57, 1 AS key#58]
+- *(2) Range (0, 11, step=1, splits=1)

Generated code:
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage2(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=2
/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean range_initRange_0;
/* 010 */   private long range_nextIndex_0;
/* 011 */   private TaskContext range_taskContext_0;
/* 012 */   private InputMetrics range_inputMetrics_0;
/* 013 */   private long range_batchEnd_0;
/* 014 */   private long range_numElementsTodo_0;
/* 015 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] range_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 016 */
/* 017 */   public GeneratedIteratorForCodegenStage2(Object[] references) {
/* 018 */     this.references = references;
/* 019 */   }
/* 020 */
/* 021 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 022 */     partitionIndex = index;
/* 023 */     this.inputs = inputs;
/* 024 */
/* 025 */     range_taskContext_0 = TaskContext.get();
/* 026 */     range_inputMetrics_0 = range_taskContext_0.taskMetrics().inputMetrics();
/* 027 */     range_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 028 */     range_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 029 */
/* 030 */   }
/* 031 */
/* 032 */   private void initRange(int idx) {
/* 033 */     java.math.BigInteger index = java.math.BigInteger.valueOf(idx);
/* 034 */     java.math.BigInteger numSlice = java.math.BigInteger.valueOf(1L);
/* 035 */     java.math.BigInteger numElement = java.math.BigInteger.valueOf(11L);
/* 036 */     java.math.BigInteger step = java.math.BigInteger.valueOf(1L);
/* 037 */     java.math.BigInteger start = java.math.BigInteger.valueOf(0L);
/* 038 */     long partitionEnd;
/* 039 */
/* 040 */     java.math.BigInteger st = index.multiply(numElement).divide(numSlice).multiply(step).add(start);
/* 041 */     if (st.compareTo(java.math.BigInteger.valueOf(Long.MAX_VALUE)) > 0) {
/* 042 */       range_nextIndex_0 = Long.MAX_VALUE;
/* 043 */     } else if (st.compareTo(java.math.BigInteger.valueOf(Long.MIN_VALUE)) < 0) {
/* 044 */       range_nextIndex_0 = Long.MIN_VALUE;
/* 045 */     } else {
/* 046 */       range_nextIndex_0 = st.longValue();
/* 047 */     }
/* 048 */     range_batchEnd_0 = range_nextIndex_0;
/* 049 */
/* 050 */     java.math.BigInteger end = index.add(java.math.BigInteger.ONE).multiply(numElement).divide(numSlice)
/* 051 */     .multiply(step).add(start);
/* 052 */     if (end.compareTo(java.math.BigInteger.valueOf(Long.MAX_VALUE)) > 0) {
/* 053 */       partitionEnd = Long.MAX_VALUE;
/* 054 */     } else if (end.compareTo(java.math.BigInteger.valueOf(Long.MIN_VALUE)) < 0) {
/* 055 */       partitionEnd = Long.MIN_VALUE;
/* 056 */     } else {
/* 057 */       partitionEnd = end.longValue();
/* 058 */     }
/* 059 */
/* 060 */     java.math.BigInteger startToEnd = java.math.BigInteger.valueOf(partitionEnd).subtract(
/* 061 */       java.math.BigInteger.valueOf(range_nextIndex_0));
/* 062 */     range_numElementsTodo_0  = startToEnd.divide(step).longValue();
/* 063 */     if (range_numElementsTodo_0 < 0) {
/* 064 */       range_numElementsTodo_0 = 0;
/* 065 */     } else if (startToEnd.remainder(step).compareTo(java.math.BigInteger.valueOf(0L)) != 0) {
/* 066 */       range_numElementsTodo_0++;
/* 067 */     }
/* 068 */   }
/* 069 */
/* 070 */   protected void processNext() throws java.io.IOException {
/* 071 */     // initialize Range
/* 072 */     if (!range_initRange_0) {
/* 073 */       range_initRange_0 = true;
/* 074 */       initRange(partitionIndex);
/* 075 */     }
/* 076 */
/* 077 */     while (true) {
/* 078 */       if (range_nextIndex_0 == range_batchEnd_0) {
/* 079 */         long range_nextBatchTodo_0;
/* 080 */         if (range_numElementsTodo_0 > 1000L) {
/* 081 */           range_nextBatchTodo_0 = 1000L;
/* 082 */           range_numElementsTodo_0 -= 1000L;
/* 083 */         } else {
/* 084 */           range_nextBatchTodo_0 = range_numElementsTodo_0;
/* 085 */           range_numElementsTodo_0 = 0;
/* 086 */           if (range_nextBatchTodo_0 == 0) break;
/* 087 */         }
/* 088 */         range_batchEnd_0 += range_nextBatchTodo_0 * 1L;
/* 089 */       }
/* 090 */
/* 091 */       int range_localEnd_0 = (int)((range_batchEnd_0 - range_nextIndex_0) / 1L);
/* 092 */       for (int range_localIdx_0 = 0; range_localIdx_0 < range_localEnd_0; range_localIdx_0++) {
/* 093 */         long range_value_0 = ((long)range_localIdx_0 * 1L) + range_nextIndex_0;
/* 094 */
/* 095 */         range_mutableStateArray_0[1].reset();
/* 096 */
/* 097 */         range_mutableStateArray_0[1].write(0, ((Decimal) references[1] /* literal */), 38, 18);
/* 098 */
/* 099 */         range_mutableStateArray_0[1].write(1, 1);
/* 100 */         append((range_mutableStateArray_0[1].getRow()));
/* 101 */
/* 102 */         if (shouldStop()) {
/* 103 */           range_nextIndex_0 = range_value_0 + 1L;
/* 104 */           ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(range_localIdx_0 + 1);
/* 105 */           range_inputMetrics_0.incRecordsRead(range_localIdx_0 + 1);
/* 106 */           return;
/* 107 */         }
/* 108 */
/* 109 */       }
/* 110 */       range_nextIndex_0 = range_batchEnd_0;
/* 111 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(range_localEnd_0);
/* 112 */       range_inputMetrics_0.incRecordsRead(range_localEnd_0);
/* 113 */       range_taskContext_0.killTaskIfInterrupted();
/* 114 */     }
/* 115 */   }
/* 116 */
/* 117 */ }

== Subtree 4 / 4 (maxMethodCodeSize:278; maxConstantPoolSize:175(0.27% used); numInnerClasses:0) ==
*(1) Project [10000000000000000000.000000000000000000 AS d#47, 1 AS key#48]
+- *(1) Range (0, 1, step=1, splits=1)

Generated code:
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean range_initRange_0;
/* 010 */   private long range_nextIndex_0;
/* 011 */   private TaskContext range_taskContext_0;
/* 012 */   private InputMetrics range_inputMetrics_0;
/* 013 */   private long range_batchEnd_0;
/* 014 */   private long range_numElementsTodo_0;
/* 015 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] range_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 016 */
/* 017 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 018 */     this.references = references;
/* 019 */   }
/* 020 */
/* 021 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 022 */     partitionIndex = index;
/* 023 */     this.inputs = inputs;
/* 024 */
/* 025 */     range_taskContext_0 = TaskContext.get();
/* 026 */     range_inputMetrics_0 = range_taskContext_0.taskMetrics().inputMetrics();
/* 027 */     range_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 028 */     range_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 029 */
/* 030 */   }
/* 031 */
/* 032 */   private void initRange(int idx) {
/* 033 */     java.math.BigInteger index = java.math.BigInteger.valueOf(idx);
/* 034 */     java.math.BigInteger numSlice = java.math.BigInteger.valueOf(1L);
/* 035 */     java.math.BigInteger numElement = java.math.BigInteger.valueOf(1L);
/* 036 */     java.math.BigInteger step = java.math.BigInteger.valueOf(1L);
/* 037 */     java.math.BigInteger start = java.math.BigInteger.valueOf(0L);
/* 038 */     long partitionEnd;
/* 039 */
/* 040 */     java.math.BigInteger st = index.multiply(numElement).divide(numSlice).multiply(step).add(start);
/* 041 */     if (st.compareTo(java.math.BigInteger.valueOf(Long.MAX_VALUE)) > 0) {
/* 042 */       range_nextIndex_0 = Long.MAX_VALUE;
/* 043 */     } else if (st.compareTo(java.math.BigInteger.valueOf(Long.MIN_VALUE)) < 0) {
/* 044 */       range_nextIndex_0 = Long.MIN_VALUE;
/* 045 */     } else {
/* 046 */       range_nextIndex_0 = st.longValue();
/* 047 */     }
/* 048 */     range_batchEnd_0 = range_nextIndex_0;
/* 049 */
/* 050 */     java.math.BigInteger end = index.add(java.math.BigInteger.ONE).multiply(numElement).divide(numSlice)
/* 051 */     .multiply(step).add(start);
/* 052 */     if (end.compareTo(java.math.BigInteger.valueOf(Long.MAX_VALUE)) > 0) {
/* 053 */       partitionEnd = Long.MAX_VALUE;
/* 054 */     } else if (end.compareTo(java.math.BigInteger.valueOf(Long.MIN_VALUE)) < 0) {
/* 055 */       partitionEnd = Long.MIN_VALUE;
/* 056 */     } else {
/* 057 */       partitionEnd = end.longValue();
/* 058 */     }
/* 059 */
/* 060 */     java.math.BigInteger startToEnd = java.math.BigInteger.valueOf(partitionEnd).subtract(
/* 061 */       java.math.BigInteger.valueOf(range_nextIndex_0));
/* 062 */     range_numElementsTodo_0  = startToEnd.divide(step).longValue();
/* 063 */     if (range_numElementsTodo_0 < 0) {
/* 064 */       range_numElementsTodo_0 = 0;
/* 065 */     } else if (startToEnd.remainder(step).compareTo(java.math.BigInteger.valueOf(0L)) != 0) {
/* 066 */       range_numElementsTodo_0++;
/* 067 */     }
/* 068 */   }
/* 069 */
/* 070 */   protected void processNext() throws java.io.IOException {
/* 071 */     // initialize Range
/* 072 */     if (!range_initRange_0) {
/* 073 */       range_initRange_0 = true;
/* 074 */       initRange(partitionIndex);
/* 075 */     }
/* 076 */
/* 077 */     while (true) {
/* 078 */       if (range_nextIndex_0 == range_batchEnd_0) {
/* 079 */         long range_nextBatchTodo_0;
/* 080 */         if (range_numElementsTodo_0 > 1000L) {
/* 081 */           range_nextBatchTodo_0 = 1000L;
/* 082 */           range_numElementsTodo_0 -= 1000L;
/* 083 */         } else {
/* 084 */           range_nextBatchTodo_0 = range_numElementsTodo_0;
/* 085 */           range_numElementsTodo_0 = 0;
/* 086 */           if (range_nextBatchTodo_0 == 0) break;
/* 087 */         }
/* 088 */         range_batchEnd_0 += range_nextBatchTodo_0 * 1L;
/* 089 */       }
/* 090 */
/* 091 */       int range_localEnd_0 = (int)((range_batchEnd_0 - range_nextIndex_0) / 1L);
/* 092 */       for (int range_localIdx_0 = 0; range_localIdx_0 < range_localEnd_0; range_localIdx_0++) {
/* 093 */         long range_value_0 = ((long)range_localIdx_0 * 1L) + range_nextIndex_0;
/* 094 */
/* 095 */         range_mutableStateArray_0[1].reset();
/* 096 */
/* 097 */         range_mutableStateArray_0[1].write(0, ((Decimal) references[1] /* literal */), 38, 18);
/* 098 */
/* 099 */         range_mutableStateArray_0[1].write(1, 1);
/* 100 */         append((range_mutableStateArray_0[1].getRow()));
/* 101 */
/* 102 */         if (shouldStop()) {
/* 103 */           range_nextIndex_0 = range_value_0 + 1L;
/* 104 */           ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(range_localIdx_0 + 1);
/* 105 */           range_inputMetrics_0.incRecordsRead(range_localIdx_0 + 1);
/* 106 */           return;
/* 107 */         }
/* 108 */
/* 109 */       }
/* 110 */       range_nextIndex_0 = range_batchEnd_0;
/* 111 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(range_localEnd_0);
/* 112 */       range_inputMetrics_0.incRecordsRead(range_localEnd_0);
/* 113 */       range_taskContext_0.killTaskIfInterrupted();
/* 114 */     }
/* 115 */   }
/* 116 */
/* 117 */ }



scala> 

